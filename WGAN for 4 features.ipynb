{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein GAN with Gradient Penalty\n",
    "## For generation of features based on LHCO2020 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py as h5\n",
    "import numpy as np\n",
    "from math import ceil, floor\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import keras.backend as K\n",
    "import gc\n",
    "print(tf.__version__)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(physical_devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    \"herwig\": \"Herwig_qcd_features.h5\",\n",
    "    \"pythiabg\": \"GAN-data\\events_anomalydetection_DelphesPythia8_v2_qcd_features.h5\",\n",
    "    \"pythiasig\": \"GAN-data\\events_anomalydetection_DelphesPythia8_v2_Wprime_features.h5\"\n",
    "}\n",
    "\n",
    "datatypes = [\"herwig\", \"pythiabg\", \"pythiasig\"]\n",
    "features = [\"px\", \"py\", \"pz\", \"m\", \"tau1\", \"tau2\", \"tau3\"]\n",
    " # Can be flexibly changed to suit GAN needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datatype, stop = None):\n",
    "    input_frame = pd.read_hdf(filenames[datatype], stop = stop)\n",
    "    output_frame = input_frame.copy()\n",
    "    for feature in features:\n",
    "        output_frame[feature + \"j1\"] = (input_frame[\"mj1\"] >= input_frame[\"mj2\"])*input_frame[feature + \"j1\"] + (input_frame[\"mj1\"] < input_frame[\"mj2\"])*input_frame[feature + \"j2\"]\n",
    "        output_frame[feature + \"j2\"] = (input_frame[\"mj1\"] >= input_frame[\"mj2\"])*input_frame[feature + \"j2\"] + (input_frame[\"mj1\"] < input_frame[\"mj2\"])*input_frame[feature + \"j1\"]\n",
    "    del input_frame\n",
    "    gc.collect()\n",
    "    output_frame[\"ej1\"] = np.sqrt(output_frame[\"mj1\"]**2 + output_frame[\"pxj1\"]**2 + output_frame[\"pyj1\"]**2 + output_frame[\"pzj1\"]**2)\n",
    "    output_frame[\"ej2\"] = np.sqrt(output_frame[\"mj2\"]**2 + output_frame[\"pxj2\"]**2 + output_frame[\"pyj2\"]**2 + output_frame[\"pzj2\"]**2)\n",
    "    output_frame[\"ejj\"] = output_frame[\"ej1\"] + output_frame[\"ej2\"]\n",
    "    output_frame[\"pjj\"] = np.sqrt((output_frame[\"pxj1\"] + output_frame[\"pxj2\"])**2 + (output_frame[\"pyj1\"] + output_frame[\"pyj2\"])**2 + (output_frame[\"pyj1\"] + output_frame[\"pyj2\"])**2)\n",
    "    output_frame[\"mjj\"] = np.sqrt(output_frame[\"ejj\"]**2 - output_frame[\"pjj\"]**2)\n",
    "    output_frame[\"tau21j1\"] = output_frame[\"tau2j1\"] / output_frame[\"tau1j1\"]\n",
    "    output_frame[\"tau32j1\"] = output_frame[\"tau3j1\"] / output_frame[\"tau2j1\"]\n",
    "    output_frame[\"tau21j2\"] = output_frame[\"tau2j2\"] / output_frame[\"tau1j2\"]\n",
    "    output_frame[\"tau32j2\"] = output_frame[\"tau3j2\"] / output_frame[\"tau2j2\"]\n",
    "\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network hyperparameters\n",
    "\n",
    "BATCH_SIZE = 1024 # Uses about 3 GB of VRAM for batch size of 512\n",
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 0.00005\n",
    "N_CRITIC = 5\n",
    "C_LAMBDA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Azure\\anaconda3\\envs\\GPU\\lib\\site-packages\\pandas\\core\\series.py:679: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df = load_data(\"herwig\")\n",
    "\n",
    "# Ensures all batches have same size\n",
    "\n",
    "df.drop([i for i in range(df.shape[0] % (BATCH_SIZE * 4))], inplace = True)\n",
    "\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df = df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[[0,1,2,4,5,6,7,8,9,11,12,13,14,15,16,17,20,22]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mj1</th>\n",
       "      <th>mj2</th>\n",
       "      <th>mjj</th>\n",
       "      <th>tau21j1</th>\n",
       "      <th>tau21j2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>559.577026</td>\n",
       "      <td>389.674988</td>\n",
       "      <td>4913.432129</td>\n",
       "      <td>0.551543</td>\n",
       "      <td>0.302850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217.050995</td>\n",
       "      <td>121.789001</td>\n",
       "      <td>3231.113770</td>\n",
       "      <td>0.680655</td>\n",
       "      <td>0.419944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130.710999</td>\n",
       "      <td>115.191002</td>\n",
       "      <td>2820.755615</td>\n",
       "      <td>0.452259</td>\n",
       "      <td>0.639433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>462.013000</td>\n",
       "      <td>42.474201</td>\n",
       "      <td>2535.666748</td>\n",
       "      <td>0.219083</td>\n",
       "      <td>0.531491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>219.542007</td>\n",
       "      <td>101.137001</td>\n",
       "      <td>4313.622070</td>\n",
       "      <td>0.217244</td>\n",
       "      <td>0.531440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999419</th>\n",
       "      <td>796.106995</td>\n",
       "      <td>32.149200</td>\n",
       "      <td>3173.365967</td>\n",
       "      <td>0.495325</td>\n",
       "      <td>0.770408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999420</th>\n",
       "      <td>333.614014</td>\n",
       "      <td>124.994003</td>\n",
       "      <td>3933.395508</td>\n",
       "      <td>0.713051</td>\n",
       "      <td>0.298112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999421</th>\n",
       "      <td>420.259003</td>\n",
       "      <td>77.524902</td>\n",
       "      <td>3262.257812</td>\n",
       "      <td>0.194199</td>\n",
       "      <td>0.600804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999422</th>\n",
       "      <td>632.781006</td>\n",
       "      <td>189.417999</td>\n",
       "      <td>3372.129150</td>\n",
       "      <td>0.209634</td>\n",
       "      <td>0.705832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999423</th>\n",
       "      <td>150.893997</td>\n",
       "      <td>100.477997</td>\n",
       "      <td>3682.832031</td>\n",
       "      <td>0.627940</td>\n",
       "      <td>0.276693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999424 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               mj1         mj2          mjj   tau21j1   tau21j2\n",
       "0       559.577026  389.674988  4913.432129  0.551543  0.302850\n",
       "1       217.050995  121.789001  3231.113770  0.680655  0.419944\n",
       "2       130.710999  115.191002  2820.755615  0.452259  0.639433\n",
       "3       462.013000   42.474201  2535.666748  0.219083  0.531491\n",
       "4       219.542007  101.137001  4313.622070  0.217244  0.531440\n",
       "...            ...         ...          ...       ...       ...\n",
       "999419  796.106995   32.149200  3173.365967  0.495325  0.770408\n",
       "999420  333.614014  124.994003  3933.395508  0.713051  0.298112\n",
       "999421  420.259003   77.524902  3262.257812  0.194199  0.600804\n",
       "999422  632.781006  189.417999  3372.129150  0.209634  0.705832\n",
       "999423  150.893997  100.477997  3682.832031  0.627940  0.276693\n",
       "\n",
       "[999424 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset consists of 732 batches of 1024 samples each, total 749568 samples\n",
      "Testset consists of 244 batches of 1024 samples each, total 249856 samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize all inputs between -1 and 1\n",
    "\n",
    "train_features = [\"mj1\",'mj2','tau21j1','tau21j2']\n",
    "scaler = MinMaxScaler((-1,1)).fit(df[train_features])\n",
    "feature_df = scaler.transform(df[train_features])\n",
    "\n",
    "X_train, X_test = train_test_split(feature_df, test_size = 0.25)\n",
    "len_dataset = int(X_train.shape[0] / BATCH_SIZE)\n",
    "len_testset = int(X_test.shape[0] / BATCH_SIZE)\n",
    "print(\"Dataset consists of {} batches of {} samples each, total {} samples\".format(len_dataset, BATCH_SIZE, len_dataset * BATCH_SIZE))\n",
    "print(\"Testset consists of {} batches of {} samples each, total {} samples\".format(len_testset, BATCH_SIZE, len_testset * BATCH_SIZE))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.642245  , -0.8018942 ,  0.02558839,  0.6040051 ],\n",
       "       [-0.88001597, -0.8767271 ,  0.1947794 ,  0.64443815],\n",
       "       [-0.689919  , -0.9274177 ,  0.1103611 ,  0.05989301],\n",
       "       ...,\n",
       "       [-0.53355825, -0.56106365,  0.16050172,  0.17293704],\n",
       "       [-0.77268004, -0.80781484,  0.11753178,  0.6995535 ],\n",
       "       [-0.7722119 , -0.92345434,  0.24101126,  0.68177044]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(np.array(X_train)).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(np.array(X_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(50, input_shape=(len(train_features),)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(50))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(50))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(len(train_features), activation='tanh'))\n",
    "    assert model.output_shape == (None, len(train_features))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(50, input_shape=(len(train_features),)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(50))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(4)) # WGAN: No sigmoid activation in last layer\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                250       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 6,154\n",
      "Trainable params: 5,854\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 50)                250       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 3,004\n",
      "Trainable params: 3,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_penalty(real, fake, epsilon): \n",
    "    # mixed_images = real * epsilon + fake * (1 - epsilon)\n",
    "    mixed_images = fake + epsilon * (real - fake)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(mixed_images) \n",
    "        mixed_scores = discriminator(mixed_images)\n",
    "        \n",
    "    gradient = tape.gradient(mixed_scores, mixed_images)[0]\n",
    "    \n",
    "    gradient_norm = tf.norm(gradient)\n",
    "    penalty = tf.math.reduce_mean((gradient_norm - 1)**2)\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def discriminator_loss(real_output, fake_output, gradient_penalty):\n",
    "    loss = tf.math.reduce_mean(fake_output) - tf.math.reduce_mean(real_output) + C_LAMBDA * gradient_penalty\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def generator_loss(fake_output):\n",
    "    gen_loss = -1. * tf.math.reduce_mean(fake_output)\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.RMSprop(lr=LEARNING_RATE)\n",
    "discriminator_optimizer = tf.keras.optimizers.RMSprop(lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensor to float for loss function plotting\n",
    "def K_eval(x):\n",
    "    try:\n",
    "        return K.get_value(K.to_dense(x))\n",
    "    except:\n",
    "        eval_fn = K.function([], [x])\n",
    "        return eval_fn([])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_generator(images):\n",
    "  noise = tf.random.normal([BATCH_SIZE, len(train_features)])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(noise, training=True)\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "\n",
    "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "\n",
    "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "  \n",
    "  return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_discriminator(images):\n",
    "  noise = tf.random.normal([BATCH_SIZE, len(train_features)])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(noise, training=True)\n",
    "\n",
    "    real_output = discriminator(images, training=True)\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "    epsilon = tf.random.normal([BATCH_SIZE,len(train_features)])\n",
    "    gp = gradient_penalty(images, generated_images, epsilon)\n",
    "    \n",
    "    disc_loss = discriminator_loss(real_output, fake_output, gp)\n",
    "\n",
    "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "  \n",
    "  return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate_generator():\n",
    "    noise = tf.random.normal([BATCH_SIZE, len(train_features)])\n",
    "    generated_images = generator(noise, training=False)\n",
    "\n",
    "    fake_output = discriminator(generated_images, training=False)\n",
    "\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate_discriminator(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, len(train_features)])\n",
    "    generated_images = generator(noise, training=False)\n",
    "\n",
    "    real_output = discriminator(images, training=False)\n",
    "    fake_output = discriminator(generated_images, training=False)\n",
    "\n",
    "    epsilon = tf.random.normal([BATCH_SIZE,len(train_features)])\n",
    "    gp = gradient_penalty(images, generated_images, epsilon)\n",
    "    \n",
    "    disc_loss = discriminator_loss(real_output, fake_output, gp)\n",
    "\n",
    "    return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_gan(generator):\n",
    "    fakedata = scaler.inverse_transform(generator(tf.random.normal((10000, len(train_features))), training=False))\n",
    "    plt.ylabel(\"Normalized to Unity\")\n",
    "    mj1=[]\n",
    "    mj2=[]\n",
    "    tau21j1=[]\n",
    "    tau21j2=[]\n",
    "    for i in fakedata:\n",
    "        mj1.append(i[0])\n",
    "        mj2.append(i[1])\n",
    "        tau21j1.append(i[2])\n",
    "        tau21j2.append(i[3])\n",
    "    plt.subplot(221)\n",
    "    plt.hist(df['mj1'], bins = 500, range = (0, 1000), color = \"tab:orange\", alpha = 0.5, label = \"Herwig Background\", density = True)\n",
    "    plt.hist(mj1, bins = 500, range = (0, 1000), color = \"tab:blue\", histtype = \"step\", label = \"GAN\", density = True)\n",
    "    plt.subplot(222)\n",
    "    plt.hist(df['mj2'], bins = 500, range = (0, 1000), color = \"tab:orange\", alpha = 0.5, label = \"Herwig Background\", density = True)\n",
    "    plt.hist(mj2, bins = 500, range = (0, 1000), color = \"tab:blue\", histtype = \"step\", label = \"GAN\", density = True)\n",
    "    plt.subplot(223)\n",
    "    plt.hist(df['tau21j1'], bins = 25, range = (0, 1), color = \"tab:orange\", alpha = 0.5, label = \"Herwig Background\", density = True)\n",
    "    plt.hist(tau21j1, bins = 25, range = (0, 1), color = \"tab:blue\", histtype = \"step\", label = \"GAN\", density = True)\n",
    "    plt.subplot(224)\n",
    "    plt.hist(df['tau21j2'], bins = 25, range = (0, 1), color = \"tab:orange\", alpha = 0.5, label = \"Herwig Background\", density = True)\n",
    "    plt.hist(tau21j2, bins = 25, range = (0, 1), color = \"tab:blue\", histtype = \"step\", label = \"GAN\", density = True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_losses = []\n",
    "train_disc_losses = []\n",
    "test_gen_losses = []\n",
    "test_disc_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_genloss():\n",
    "    plt.title(\"Generator Loss\")\n",
    "    plt.ylabel(\"Wasserstein Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.plot(train_gen_losses, 'b', label = \"Training loss\")\n",
    "    plt.plot(test_gen_losses, 'r', label = \"Validation loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_discloss():\n",
    "    plt.title(\"Discriminator Loss\")\n",
    "    plt.ylabel(\"Wasserstein Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.plot(train_disc_losses, 'b', label = \"Training loss\")\n",
    "    plt.plot(test_disc_losses, 'r', label = \"Validation loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, testset, epochs, n_critic):\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    print_losses = False #((epoch + 10) % 20 == 0)\n",
    "    draw_outputs = ((epoch + 1) % 10 == 0)\n",
    "\n",
    "    train_gen_loss = 0\n",
    "    train_disc_loss = 0\n",
    "\n",
    "    test_gen_loss = 0\n",
    "    test_disc_loss = 0\n",
    "\n",
    "    # Training\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_gen_loss += K_eval(train_step_generator(image_batch))\n",
    "      for n in range(n_critic):\n",
    "        train_disc_loss += K_eval(train_step_discriminator(image_batch))\n",
    "    \n",
    "    train_gen_losses.append(train_gen_loss / len_dataset)\n",
    "    train_disc_losses.append(train_disc_loss / len_dataset / n_critic)\n",
    "    print(train_gen_loss)\n",
    "    print(train_disc_loss)\n",
    "    print(len_dataset)\n",
    "\n",
    "    # Evaluation\n",
    "\n",
    "    for test_batch in testset:\n",
    "      test_gen_loss += K_eval(evaluate_generator())\n",
    "      test_disc_loss += K_eval(evaluate_discriminator(test_batch))\n",
    "\n",
    "    test_gen_losses.append(test_gen_loss / len_testset)\n",
    "    test_disc_losses.append(test_disc_loss / len_testset)\n",
    "    print(test_gen_loss)\n",
    "    print(test_disc_loss)\n",
    "    print(len_testset)\n",
    "\n",
    "    # Logging\n",
    "    \n",
    "    if print_losses:\n",
    "      print()\n",
    "\n",
    "      print(\"Epoch \" + str(epoch + 1) + \":\")\n",
    "      print()\n",
    "      print(\"Generator training loss: \" + str(train_gen_losses[-1]))\n",
    "      print(\"Discriminator training loss: \" + str(train_disc_losses[-1]))\n",
    "      print()\n",
    "      print(\"Generator validation loss: \" + str(test_gen_losses[-1]))\n",
    "      print(\"Discriminator validation loss: \" + str(test_disc_losses[-1]))\n",
    "\n",
    "    if draw_outputs:\n",
    "      print()\n",
    "      print(\"Epoch \" + str(epoch + 1) + \":\")\n",
    "      graph_gan(generator)\n",
    "\n",
    "      graph_genloss()\n",
    "      graph_discloss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step_generator at 0x000001F13C7CB4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <function train_step_generator at 0x000001F13C7CB4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:AutoGraph could not transform <function generator_loss at 0x000001F13C7C58B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <function generator_loss at 0x000001F13C7C58B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:AutoGraph could not transform <function train_step_discriminator at 0x000001F117267C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <function train_step_discriminator at 0x000001F117267C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:AutoGraph could not transform <function gradient_penalty at 0x000001F13C7C50D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: AutoGraph could not transform <function gradient_penalty at 0x000001F13C7C50D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function discriminator_loss at 0x000001F13C7C5558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <function discriminator_loss at 0x000001F13C7C5558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "train(train_dataset, test_dataset, EPOCHS, N_CRITIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
